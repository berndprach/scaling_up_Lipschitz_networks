# Scaling Up Lipschitz Networks
This repository contains code and results for our project *Scaling Up Lipschitz Networks*,
where we explored how scaling up training data and training time impacts the performance of 
two standard 1-Lipschitz models.

![ResultsPlot](https://berndprach.github.io/images/blog-posts/scaling_up.png)

## Introduction

Recent research has consistently shown that expanding training datasets greatly improves 
robust classification accuracy.
In this project we use huge amounts of training
[datas generated by a diffusion model](https://github.com/wzekai99/DM-Improves-AT),
to train two standard 1-Lipschitz architectures:
A [CPL](https://proceedings.mlr.press/v162/meunier22a/meunier22a.pdf) ConvNet and an
[AOL](https://arxiv.org/pdf/2208.03160) MLP.

We trained on up to 20 million generated examples, and trained for up to
8 days (on a GPU A100). Our main result is that scaling up data
and training time enables a simple MLP to approach state-of-the-art performance.


## Key results
### CPL ConvNet:
- Surprisingly, the best hyperparameter configuration barely depends on the training time.
- We managed to get to about **74%** certified robust test accuracy for perturbation radius $$36 / 255$$.

### AOL MLP:
- We get great performance despite the simple architecture. (*Fully connected layers is all you need?*)
- Large batch size (e.g. 2048) worked well.
- A width equal to the input image size ($$3072$$) seems optimal independent of the training time.
- Increasing the depth for longer training runs improved performance.
- Our best model achieves **77.6%** certified robust test accuracy (radius $$36 / 255$$) 
after $$8$$ days of training.
- Interestingly, the optimal learning rate depended on the batch size linearly in $$\sqrt{b}$$
(maintaining constant gradient variance).

## Conclusion
Scaling up is a great strategy to improving robust performance. 
To do this efficiently, we need to optimize *improvement per second*. 
We found that e.g. increasing the batch size had this effect, 
and expect that further optimizing this is a promising way to 
advance robust classification performance.

## Future Directions
- **Finetuning on original CIFAR-10 images**: 
We havenâ€™t yet incorporated the original training data, 
and expect that doing so could further boost performance.

- **Adjusting power iteration steps for CPL**: 
We kept this parameter fixed. Increasing it may enable larger batch sizes and improved results.


## Reproducing the results:
The pseudocode below reproduces some of the results:

### Setting up:
```
python -m venv .venv
source .venv/bin/activate
python -m pip install -e .

python lipschitz/scripts/basic/download_data.py "{'name': 'EDMCIFAR10', 'version': '20M'}"
```


### Finding best batch size:
```
python lipschitz/scripts/basic/time_batch_all.py -r="{'dataset.batch_size': [2**i for i in range(4, 12)]}" -u="{'model': {'name': 'AOL-MLP', 'width': 3072, 'number_of_layers': 8}, 'dataset': {'name': 'IntegerEDMCIFAR10', 'version': '20M'}, 'preprocessing.name': 'convert_and_center'}" -n=1000
for BATCH_SIZE in [2**i for i in range(4, 12)]:
    for _ in range(12):
        sbatch --array=1-12 array_run.sh python lipschitz/scripts/train/timed.py -exp="AOL-BS" -t=1. -u="{'optimizer.lr': 10**random.uniform(-2, 0), 'model': {'name': 'AOL-MLP', 'width': 3072, 'number_of_layers': 8}, 'dataset': {'name': 'IntegerEDMCIFAR10', 'version': '20M', 'batch_size': BATCH_SIZE}, 'preprocessing.name': 'convert_and_center'}"
python lipschitz/scripts/plot/results.py -c="{'arguments.experiment_name': 'AOL-BS', 'arguments.time_in_hours': 1.}" -s="configuration.dataset.batch_size" -x="configuration.optimizer.lr" -xs=log
```


### Test set experiments:
```
sbatch gpu100_array_run.sh python lipschitz/scripts/basic/time_batch_all.py -r="{'model.depth': [16]}" -u="{'model': {'name': 'AOL-MLP', 'width': 3072}, 'dataset': {'name': 'IntegerEDMCIFAR10', 'version': '20M', 'batch_size': 2048, 'use_test_data': True}, 'preprocessing.name': 'convert_and_center'}" -n=1000
for TRAINING_TIME in [0.75, 3, 12, 48, 192]:
    python lipschitz/scripts/train/timed.py -t=TRAINING_TIME -exp="FINAL20M" -u="{'augmentation': {'name': 'color_crop_flip_erase', 'crop_size': 4, 'erase_proportion': 0.}, 'optimizer.lr': 0.3, 'model': {'name': 'AOL-MLP', 'width': 3072, 'depth': 16}, 'dataset': {'name': 'IntegerEDMCIFAR10', 'version': '20M', 'batch_size': 2048, 'use_test_data': True}, 'preprocessing.name': 'convert_and_center'}"
python lipschitz/scripts/plot/results.py -c="{'arguments.experiment_name': 'FINAL20M'}" -s="configuration.model.depth" -x="arguments.time_in_hours" -xs=log
```

## Work done at

<a href="https://ist.ac.at/en/research/lampert-group/">
<img src="https://berndprach.github.io/images/ISTA_Logo.png" width="160">
</a>